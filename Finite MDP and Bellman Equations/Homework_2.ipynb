{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<center>\n",
    "Reinforcement Learning\n",
    "\n",
    "# Homework 2 - Finite MDPs and Bellman Equations\n",
    "\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**General Notes:**\n",
    "- When writing your answers, you should use proper math symbols as explained in the markdown section of Jupyter lecture. For example: $V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q1. In this question, we want to show the effect of adding a constant reward to all rewards on the value of the states. Prove that adding a constant $c$ to all rewards adds a constant ($V_c$)  to the value of all states, and thus does not affect the relative values of any states under any policies. What is $V_c$ in terms of $c$ and $\\gamma$? \n",
    "\n",
    "**Hint**: start from $G_t = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$ and calculate $V_{\\pi}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">Answer: <br>$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+1+k} $ <br> The modified $G_t$ after adding a constant $c$ is:\n",
    "$G_t^c = \\sum_{k=0}^{\\infty} \\gamma^k \\left(R_{t+1+k} + c\\right)\n",
    "= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+1+k} + \\sum_{k=0}^{\\infty} c\\cdot \\gamma^k \n",
    "= G_t + \\frac{c}{1 - \\gamma}=G_t+V_c$ <br>\n",
    "The modified $V_{\\pi}(s)$ is: <br> \n",
    "$V_{\\pi}^c(s) = \\mathbb{E}_{\\pi}[G_t^c | S_t = s]=V_c+V_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q2. Suppose $\\gamma=0.5$ and the following sequence of rewards is received: $R_1=-1, R_2=2, R_3=6, R_4=3, R_5=2$, with $T=5$. <br> What are $G_0, G_1, ..., G_5$? \n",
    "\n",
    "**Hint**: Work backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">Answer:<br>As we know:<br>$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+1+k} =R_{t+1}+ \\sum_{k=0}^{\\infty} \\gamma^{k+1} R_{t+1+k+1}=R_{t+1}+ \\gamma\\sum_{k=0}^{\\infty} \\gamma^{k} R_{(t+1)+1+k}=R_{t+1}+ \\gamma G_{t+1}$ <br> Therefore, the answer is as follows: <br>\n",
    "$G_5=R_6+\\gamma G_7=0$ <br>\n",
    "$G_4=R_5+\\gamma G_5=2$ <br>\n",
    "$G_3=R_4+\\gamma G_4=4$ <br>\n",
    "$G_2=R_3+\\gamma G_3=8$ <br>\n",
    "$G_1=R_2+\\gamma G_2=6$ <br>\n",
    "$G_0=R_1+\\gamma G_1=2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q3. We have derived Bellman equation for $V_{\\pi}(s)$ that expresses the relationship between the value of a state and the values of its successor states (backup diagram included in the slides). What is the Bellman equation for $Q_{\\pi} (s,a)$? It must give action value $Q_{\\pi} (s,a)$ in terms of $Q_{\\pi} (s',a')$. Use the following backup diagram to write the equation and explain its individual terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Answer:  <br> \n",
    "$Q_{\\pi}(s,a)= \\mathbb{E}_{\\pi}[G_t | S_t = s,A_t=a]=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}| S_t = s,A_t=a]\n",
    "=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\sum_{a'}\\pi(a'|s')Q(s',a')]$ <br>\n",
    "Here, $p(s',r|s,a)$ is the probability of being at state $s'$ and get a reward $r$, given the current state $s$ and the action $a$, $\\gamma$ is the discount factor, $Q(s',a')$ is the action value of state $s'$ for a action $a'$. However, as the action $a'$ is not fixed, we take the expected value over the all possible action using policy $\\pi$, which is reflected in term $\\pi(a'|s')$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q4. Consider the continuing MDP shown below. The only decision to be made is that in the top state, where two actions are available, left and right. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, $\\pi_L$,$\\pi_R$. What policy is optimal if\n",
    "1. $\\gamma=0$\n",
    "2. $\\gamma = 0.9$\n",
    "3. $\\gamma = 0.5$\n",
    "\n",
    "Explain your computation for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">Answer:<br>\n",
    "States= {A,B,C} # anti-clockwise in the above figure <br>\n",
    "Now, \n",
    "$Q_{\\pi}(A,\\pi_L)=1+\\gamma Q_{\\pi}(B,a)=1+\\gamma [0+Q_{\\pi}(A,\\pi_L)]$\n",
    ",which implies: $Q_{\\pi}(A,\\pi_L) = \\frac{1}{1-\\gamma ^2}$<br>\n",
    "Similarly,\n",
    "$Q_{\\pi}(A,\\pi_R) = \\frac{2\\gamma}{1-\\gamma ^2}$<br>\n",
    "For $\\gamma=0, Q_{\\pi}(A,\\pi_L)= 1; Q_{\\pi}(A,\\pi_R)= 0;  \\pi_L$ is optimal <br>\n",
    "For $\\gamma = 0.9, Q_{\\pi}(A,\\pi_L)= \\frac{100}{19}; Q_{\\pi}(A,\\pi_R)= \\frac{180}{19}; \\pi_R$ is optimal <br>\n",
    "For $\\gamma = 0.5, Q_{\\pi}(A,\\pi_L)= \\frac{4}{3}; Q_{\\pi}(A,\\pi_R)= \\frac{4}{3}; $  Both policies are optimal <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db33ea9fa7b2c650cb0566c85cffe253d425b7cca18759b45c6b5f450d97eb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
